{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksharat/FinanceProjects/blob/main/Template_Synthetic_Identity_Fraud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What Is Synthetic Identity Fraud?**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "* Synthetic identity fraud is a type of identity theft in which criminals create fictitious identities by combining real and fake information. Unlike traditional identity theft, where the criminal assumes the identity of a real person, synthetic identity fraud involves the fabrication of entirely new identities.\n",
        "\n",
        "* Synthetic identity fraud is a complex and evolving form of identity theft that poses significant challenges for law enforcement agencies, financial institutions, and individuals. Detecting and preventing this type of fraud requires a multi-layered approach that involves robust identity verification processes, improved data security measures, and increased awareness among individuals to safeguard their personal information.\n",
        "\n",
        "This notebook presents a Python-based solution to combat synthetic identity fraud."
      ],
      "metadata": {
        "id": "jGCca5uryCH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TABLE OF CONTENTS**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        ">[What Is Synthetic Identity Fraud?](#scrollTo=jGCca5uryCH7)\n",
        "\n",
        ">[Setup: Library Imports](#scrollTo=yDtNLyGAMarO)\n",
        "\n",
        ">[Load Dataset](#scrollTo=Cu4mZjOcMarS)\n",
        "\n",
        ">[Data Exploration](#scrollTo=2Is3sWpNWPpF)\n",
        "\n",
        ">>[Data Visualization](#scrollTo=is0BrIHJ-kT2)\n",
        "\n",
        ">[Data Cleaning](#scrollTo=59hF-jqZXebA)\n",
        "\n",
        ">[Data Preparation](#scrollTo=7Q_iDr_rMarT)\n",
        "\n",
        ">[Test/Train Split Overview](#scrollTo=zVKTBgO6Nh9z)\n",
        "\n",
        ">>[Why Create A Split Based On Months?](#scrollTo=zVKTBgO6Nh9z)\n",
        "\n",
        ">[Model Evaluation](#scrollTo=AqCEwHsNMarT)\n",
        "\n",
        ">>[But WHY are we using these metrics?](#scrollTo=0T2Aaa0Un9Fa)\n",
        "\n",
        ">[Model Building](#scrollTo=Qx8Mx6YYMarV)\n",
        "\n",
        ">>[Algorithm Selection](#scrollTo=Pdbh5ZwEnvbN)\n",
        "\n",
        ">>>[Simplest Solution](#scrollTo=8bw3LbQTn30Y)\n",
        "\n",
        ">>>[Medium-level Algorithm Selection](#scrollTo=GWvgYnB_9l_E)\n",
        "\n",
        ">>>[High Complexity Solution](#scrollTo=AqSmTpVN-Nr_)\n",
        "\n",
        ">>>[Deep Learning Approach](#scrollTo=PjE5xqv79Kwr)\n",
        "\n",
        ">[Analyzing Model Performance](#scrollTo=KOlWnsQQlS5i)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "2sNJl-HUpQjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup: Library Imports**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yDtNLyGAMarO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NumPy (Numerical Python) is a powerful library in Python that provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently. It is a fundamental package for scientific computing in Python.\n",
        "\n",
        "* [Numpy Reference Material](https://www.w3schools.com/python/numpy/numpy_intro.asp)\n",
        "\n",
        "* [Numpy Video Tutorial](https://www.youtube.com/watch?v=QUT1VHiLmmI&ab_channel=freeCodeCamp.org\n",
        ")\n",
        "\n",
        "\n",
        "Pandas is a powerful open-source library in Python that provides high-performance data manipulation and analysis tools. It is built on top of NumPy and provides an easy-to-use data structure called DataFrame, which allows for efficient handling and manipulation of structured data.\n",
        "\n",
        "* [Pandas reference material](https://www.w3schools.com/python/pandas/default.asp)\n",
        "\n",
        "* [Pandas video tutorial](https://www.youtube.com/watch?v=vmEHCJofslg&ab_channel=KeithGalli\n",
        ")\n",
        "\n",
        "---\n",
        "These imports may seem overwhelming at first glance, but each library will be explained in detail before its use to ensure a clear understanding of their purpose and functionality."
      ],
      "metadata": {
        "id": "QwOgKoaNBhC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2023-01-10T21:35:11.09468Z",
          "iopub.execute_input": "2023-01-10T21:35:11.095069Z",
          "iopub.status.idle": "2023-01-10T21:35:11.110963Z",
          "shell.execute_reply.started": "2023-01-10T21:35:11.095036Z",
          "shell.execute_reply": "2023-01-10T21:35:11.109775Z"
        },
        "trusted": true,
        "id": "E7OH8GX9MarQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-01-10T21:35:11.17613Z",
          "iopub.execute_input": "2023-01-10T21:35:11.177391Z",
          "iopub.status.idle": "2023-01-10T21:35:20.982823Z",
          "shell.execute_reply.started": "2023-01-10T21:35:11.17735Z",
          "shell.execute_reply": "2023-01-10T21:35:20.981611Z"
        },
        "trusted": true,
        "id": "KiaDemp-MarS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0eVpmdZ-TuY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Cu4mZjOcMarS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ojy92NQPUU7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2023-01-10T21:35:20.985066Z",
          "iopub.execute_input": "2023-01-10T21:35:20.985712Z",
          "iopub.status.idle": "2023-01-10T21:35:24.041736Z",
          "shell.execute_reply.started": "2023-01-10T21:35:20.98567Z",
          "shell.execute_reply": "2023-01-10T21:35:24.040633Z"
        },
        "trusted": true,
        "id": "zg05AtjLMarS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vr0HqpPq8E5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Exploration**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Tasks:**\n",
        "1. Data Types\n",
        "2. Unique Values\n",
        "3. Distribution of the values of features\n",
        "4. Feature statistics\n",
        "5. Identify outliers\n",
        "6. Correlation between features\n",
        "7. Identify missing data"
      ],
      "metadata": {
        "id": "2Is3sWpNWPpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pandas library offers an exploratory data visualization tool through Profiling Report.\n",
        "\n",
        "Profiling Report includes:\n",
        "\n",
        "* **Overview**: Basic information about the dataset, such as the number of variables, unique values, observations, and missing values.\n",
        "\n",
        "* **Variable types**: Identification of the data types of each variable, such as numerical, categorical, or date/time.\n",
        "\n",
        "* **Correlations**: Analysis of correlations between pairs of variables, helping to identify any strong relationships or dependencies.\n",
        "\n",
        "* **Missing values**: Identification of variables with missing values and the percentage of missing values in each variable.\n",
        "\n",
        "* **Interaction**: Identification of potential interactions between variables.\n",
        "\n",
        "Profiling Report also provides **visual representations of the data**:\n",
        "* **Bar Graph**: A visual representation using rectangular bars to show the frequency or count of categorical data. It is commonly used for comparing and displaying categorical data, such as sales performance of different products or customer preferences across categories.\n",
        "\n",
        "  [Text and Video Resource exploring Bar Graphs in Python ](https://dataindependent.com/pandas/pandas-bar-plot-dataframe-plot-bar/)\n",
        "\n",
        "* **Heat Map**: A graphical representation using colors to display data values in a matrix or table format. It is useful for visualizing relationships or patterns in large datasets, particularly for showing correlations or associations between variables.\n",
        "\n",
        "    [Short video explanation on how to interpret Heat Maps](https://www.youtube.com/watch?v=VmlO-GIvEek)\n",
        "\n",
        "* **Matrix**: A tabular representation of data with rows and columns displaying variables or categories. It is used to display summary statistics, similarity measures, or correlation coefficients between variables.\n",
        "\n",
        "    [Article explaining correlation matrices](https://www.displayr.com/what-is-a-correlation-matrix/)\n",
        "\n",
        "    [Detailed text resource for creating correlation matrices on Pandas](https://datatofish.com/correlation-matrix-pandas/)\n",
        "\n",
        "* **Histogram**: A graphical representation showing the distribution of numerical data through intervals or bins. It helps in understanding the distribution and shape of a numerical variable, such as identifying patterns or skewness in exam scores.\n",
        "\n",
        "  [Detailed text resource exploring histograms on Pandas](https://sparkbyexamples.com/pandas/pandas-plot-a-histogram/)\n",
        "\n",
        "  [Short video tutorial for creating histograms on Pandas](https://www.youtube.com/watch?v=ra2pw0qKWvg)\n",
        "\n",
        "\n",
        "**Additional Resources On Profiling Report**:\n",
        "\n",
        "* [YouTube video outlining Profiling Report](https://www.youtube.com/watch?v=Ef169VELt5o)\n",
        "* [Text Resource on Profiling Report](https://towardsdatascience.com/pandas-profiling-easy-exploratory-data-analysis-in-python-65d6d0e23650)\n"
      ],
      "metadata": {
        "id": "qWvS1nsF9hRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Exploration\n",
        "\n"
      ],
      "metadata": {
        "id": "wtqzu75tKyHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PTXtiLfrKyjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Documentation on Visualization Library\n",
        "---\n",
        "\n",
        "\n",
        "Matplotlib is a widely-used data visualization library in Python. It provides a variety of functions and classes for creating high-quality static, animated, and interactive plots. Matplotlib allows you to create a wide range of visualizations, including line plots, scatter plots, bar plots, histograms, heatmaps, contour plots, and more.\n",
        "\n",
        "Additional Resources:\n",
        "* [Brief Matplotlib Tutorial](https://www.youtube.com/watch?v=a9UrKTVEeZA)\n",
        "* [Extensive Matplotlib Tutorial ](https://www.youtube.com/watch?v=UO98lJQ3QGI&list=PL-osiE80TeTvipOqomVEeZ1HRrcEvtZB_)\n",
        "*   [Matplotlib Video Playlist](https://www.youtube.com/watch?v=UO98lJQ3QGI&list=PL-osiE80TeTvipOqomVEeZ1HRrcEvtZB_)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Seaborn is a popular data visualization library in Python that is built on top of Matplotlib. It provides a high-level interface for creating aesthetically pleasing and informative statistical graphics. Seaborn simplifies the process of creating visually appealing plots and offers a wide range of built-in features and customization options.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "is0BrIHJ-kT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The plot generated by the following code block uses a bar graph to visualize the distribution of fraudulent and legitimate transactions, helping us understand the frequency or occurrence of each type of transaction."
      ],
      "metadata": {
        "id": "M-fGRfVxViwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Any additional visualizations\n",
        "\n"
      ],
      "metadata": {
        "id": "qw6T8KfyQMxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c6EDGQUmK9Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also visualize the missing values for each variable and observe how the distribution of fraudulent and legitimate transactions is related to these missing values."
      ],
      "metadata": {
        "id": "3BoMoFNlVrYb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VBaSsrxpRS4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9Enlk-XRKnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Dt50zgdRFy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot can provide insights into any potential patterns or associations between missing data and the transaction types."
      ],
      "metadata": {
        "id": "TtI_UAYuaPrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Cleaning**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "59hF-jqZXebA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(axis=0)\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBZrpFQZOLqo",
        "outputId": "0dd8991c-286e-43e0-a2ce-f786f8a0ca1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000000, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into features and target\n",
        "X = df.drop(['fraud_bool'], axis=1)\n",
        "y = df['fraud_bool']"
      ],
      "metadata": {
        "id": "2oJgrJ2bZTQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Data Preparation**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Task:** Categorical Features: OneHotEncoding on all the categorical features\n",
        "\n",
        "**One-hot encoding** is a technique used to represent categorical variables as binary vectors in machine learning and data analysis. It is commonly used when working with categorical data that cannot be directly used as input for certain models or algorithms.\n",
        "\n",
        "**Material:**\n",
        "\n",
        "[Text article exploring One-hot encoding in Python](https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/)\n",
        "\n",
        "[Youtube video](https://www.youtube.com/watch?v=i2JSH5tn2qc\n",
        ")\n",
        "\n",
        "The **process of one-hot encoding** involves converting each categorical value into a binary vector where only one bit is \"on\" (1) while the rest are \"off\" (0). This binary vector representation allows the categorical variable to be used as input in numerical computations."
      ],
      "metadata": {
        "id": "7Q_iDr_rMarT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-10T21:35:24.52282Z",
          "iopub.execute_input": "2023-01-10T21:35:24.523194Z",
          "iopub.status.idle": "2023-01-10T21:35:24.585157Z",
          "shell.execute_reply.started": "2023-01-10T21:35:24.523158Z",
          "shell.execute_reply": "2023-01-10T21:35:24.584011Z"
        },
        "trusted": true,
        "id": "3BX378ooMarU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-10T21:35:24.587066Z",
          "iopub.execute_input": "2023-01-10T21:35:24.587472Z",
          "iopub.status.idle": "2023-01-10T21:35:26.596899Z",
          "shell.execute_reply.started": "2023-01-10T21:35:24.587433Z",
          "shell.execute_reply": "2023-01-10T21:35:26.595919Z"
        },
        "trusted": true,
        "id": "Zox7tVqSMarU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test/Train Split Overview**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We split our data into a feature data frame and target variable column.\n",
        "\n",
        "We perform a **Test/Train Split** on the data, dividing it into a training set and a testing set. The training set is used to train the model, while the testing set evaluates its performance on new data. This split allows the model to learn patterns and make predictions, while the testing set assesses its ability to generalize and make accurate predictions. It simulates real-world scenarios, measuring the model's performance and its capability to handle unseen data.\n",
        "## Why Create A Split Based On Months?\n",
        "Real-world datasets are susceptible to biases and variations over time, which can impact the distribution of features. In dynamic environments like fraud detection, fraudsters constantly adapt their behavior to avoid being caught. This means that the features that were effective in detecting fraud before may become ineffective as fraudsters find new ways to evade detection.\n",
        "\n",
        "To account for potential variations over time, we split the data into training and test sets based on months. The training set includes data from January to May, while the test set comprises data from June to July. This division helps us assess how well the model performs on new data and generalize beyond the training period.\n",
        "\n",
        "Additional Resources on Test/Train Split:\n",
        "*   [Article with in-depth description of test-train split](https://builtin.com/data-science/train-test-split)\n",
        "*   [Video explanation of test-train split ](https://www.youtube.com/watch?v=BAiMKBrFntc)"
      ],
      "metadata": {
        "id": "zVKTBgO6Nh9z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9jA0zqToftWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9z3s8oaNLHOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mkdOmsbPLKlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Numerical Feature Standardization**\n",
        "\n",
        "We will standardize the numerical features of our train and test set. We will be using scikit-learn, a popular open-source machine learning library for Python, specifically the StandardScaler class, which transforms numerical data by subtracting the mean and dividing by the standard deviation, resulting in features with zero mean and unit variance.\n",
        "\n",
        "This ensures that ensures that the transformed features have comparable scales, making them suitable for machine learning models that are sensitive to the scale of the input data.\n",
        "\n",
        "Here are some resources so that you may develop a broader understanding of Scikit-learn:\n",
        "\n",
        "*  [Scikit-learn Guide](https://scikit-learn.org/stable/)\n",
        "\n",
        "*  [In-depth video explanation of Scikit-learn](https://www.youtube.com/watch?v=pqNCD_5r0IU)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "37StM_ES0-gT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-10T21:35:26.598424Z",
          "iopub.execute_input": "2023-01-10T21:35:26.599068Z",
          "iopub.status.idle": "2023-01-10T21:35:27.603659Z",
          "shell.execute_reply.started": "2023-01-10T21:35:26.599029Z",
          "shell.execute_reply": "2023-01-10T21:35:27.602604Z"
        },
        "trusted": true,
        "id": "COKJ_cQTMarU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dputJxrnLMT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Evaluation**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AqCEwHsNMarT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-10T21:35:24.066192Z",
          "iopub.execute_input": "2023-01-10T21:35:24.06681Z",
          "iopub.status.idle": "2023-01-10T21:35:24.521276Z",
          "shell.execute_reply.started": "2023-01-10T21:35:24.066776Z",
          "shell.execute_reply": "2023-01-10T21:35:24.519287Z"
        },
        "trusted": true,
        "id": "sLhG11w4MarT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "81XeHorlLOMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting ROC Curve Method**\n",
        "* The **ROC** curve illustrates the relationship between TPR (true positive rate) and FPR (false positive rate) at various classification thresholds, or decision boundaries used to determine class label. An ideal ROC curve exhibits high values in its leftmost quadrant, signifying a high TPR and low FPR.\n",
        "\n",
        "**Fairness Metrics Method**\n",
        "\n",
        "* Evaluates the fairness of a classification model's predictions across different groups or demographics.\n",
        "* Using the aequitas library, a lightweight library for model fariness analytics, the method builds a **confusion matrix**, a table that summarizes the performance of a classification model by displaying the counts of true positives, true negatives, false positives, and false negatives.\n",
        "* The method also calculates the Predictive_equality, which is the difference between the disparities in the false positive rate. Ideally, this value should be low or zero.\n",
        "* **Additional Resources:**\n",
        "  * [Basic Introduction To Fairness in Machine Learning](https://www.youtube.com/watch?v=euwc0va-7Vo)\n",
        "\n",
        "\n",
        "\n",
        "**Evaluation Function Concepts**\n",
        "* The Evaluation function ties the other two methods together. It calculates predictive equality and plots the ROC, as well as the **AUC** or \"area under the curve,\" which summarizes the ROC curve, representing the model's overall performance. Higher AUC indicates better discrimination ability, while 0.5 suggests random performance and 1 represents a perfect model."
      ],
      "metadata": {
        "id": "tDaCCYw-MarU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-10T21:35:27.605203Z",
          "iopub.execute_input": "2023-01-10T21:35:27.605815Z",
          "iopub.status.idle": "2023-01-10T21:35:27.662439Z",
          "shell.execute_reply.started": "2023-01-10T21:35:27.605774Z",
          "shell.execute_reply": "2023-01-10T21:35:27.661592Z"
        },
        "trusted": true,
        "id": "8LWUdxwoMarU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-10T21:35:27.663742Z",
          "iopub.execute_input": "2023-01-10T21:35:27.664206Z",
          "iopub.status.idle": "2023-01-10T21:35:27.669697Z",
          "shell.execute_reply.started": "2023-01-10T21:35:27.664169Z",
          "shell.execute_reply": "2023-01-10T21:35:27.668709Z"
        },
        "trusted": true,
        "id": "F2WDeTgBMarV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# But WHY are we using these metrics?\n",
        "\n",
        "##Storytime\n",
        "**Imagine this:** You've saved up for an exciting vacation in a foreign country. Having taken your family out for a great dinner at a seaside restaurant, you couldn't imagine anything going wrong. But when you hand your card to the waiter, disaster strikes. The bank notices an unexpected transaction in a different country and flags your delicious dinner as fraud! As the waiter loudly explains that your card has been declined, your face turns bright red with embarrassment as you feel a growing frustration towards your bank.\n",
        "\n",
        "\\\n",
        "*Well what does this have to do with the problem we are trying to solve?*\n",
        "\n",
        "\\\n",
        "Customers strongly dislike being falsely flagged for fraud. Incorrectly labeling a customer as a fraudster is a surefire method to lose them as a client.\n",
        "\n",
        "\\\n",
        "## The Need For Real World Metrics\n",
        "**Ultimately**, it's best to use metrics that fit into place in the real world. For example, a common model metric such as \"accuracy\" would fall short in this problem context. While accuracy incorporates both true positives and true negatives, it treats false positives and false negatives equally. In the context of fraud detection, false positives (incorrectly flagging legitimate transactions) tend to have more immediate and tangible consequences compared to false negatives (missed fraudulent transactions). Therefore, optimizing FPR becomes crucial to minimize the negative impact on customers and the business.\n",
        "\n",
        "\\\n",
        "FPR and TPR offer a more targeted evaluation, allowing companies to optimize their fraud detection systems to strike a balance between customer satisfaction, revenue protection, and risk mitigation."
      ],
      "metadata": {
        "id": "0T2Aaa0Un9Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-10T21:35:27.67109Z",
          "iopub.execute_input": "2023-01-10T21:35:27.671676Z",
          "iopub.status.idle": "2023-01-10T21:35:27.682094Z",
          "shell.execute_reply.started": "2023-01-10T21:35:27.671641Z",
          "shell.execute_reply": "2023-01-10T21:35:27.681074Z"
        },
        "trusted": true,
        "id": "8trcZ3eUMarV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rsYvEnX0LRVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Building**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qx8Mx6YYMarV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm Selection"
      ],
      "metadata": {
        "id": "Pdbh5ZwEnvbN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "su4pP5LRHPR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uhTvxb2LLSwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "**Classification**:\n",
        "Machine learning classification is a supervised learning technique that focuses on categorizing or assigning discrete class labels to input data based on their features and patterns. Classification algorithms aim to generalize the patterns observed in the training data to make accurate predictions on unseen data, enabling automated decision-making and pattern recognition in various domains.\n",
        "\n",
        "**Binary Classification:** This type of classification involves dividing the data into two distinct classes. Examples include classifying emails as spam or not spam, predicting whether a customer will churn or not, or determining if a credit card transaction is fraudulent or legitimate.\n",
        "\n",
        "**Multiclass Classification:** In multiclass classification, the data is divided into more than two classes. Each instance is assigned to one and only one class. Examples include classifying images into different object categories, recognizing handwritten digits, or classifying news articles into different topics.\n",
        "\n",
        "**Imbalanced Classification:** Imbalanced classification refers to situations where the classes are not represented equally in the dataset. Typically, one class (majority class) has a significantly larger number of instances compared to the other class(es) (minority class(es)). Imbalanced classification algorithms are specifically designed to handle such scenarios, where the focus is on accurately identifying the minority class.\n",
        "\n",
        "**Multi-label Classification:** In multi-label classification, each instance can be assigned multiple labels or categories simultaneously. This means that an instance can belong to more than one class. Examples include assigning tags to documents, predicting the presence of multiple diseases in medical diagnosis, or labeling images with multiple objects present.\n",
        "\n",
        "**Material**:\n",
        "\n",
        "[In-depth text explanation of Classification](https://www.simplilearn.com/tutorials/machine-learning-tutorial/classification-in-machine-learning)\n",
        "\n",
        "[Youtube video on Classification ](https://www.youtube.com/watch?v=xG-E--Ak5jg)\n",
        "\n",
        "\n",
        "We have provided with you three distinct solutions to our synthetic identity fraud problem which you can explore through the rest of this module.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kuxxY6lJ8nO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Simplest Solution**\n",
        "\n",
        "For the simplist solution, we have decided to approach this problem through a Classification algorithm known as Logistic Regression.\n",
        "\n",
        "\n",
        "**Logistic Regression** is a classification algorithm that predicts the probability of an instance belonging to a specific class using the logistic function. It is widely used for binary classification tasks, offering simplicity and interpretability in modeling the relationship between input features and class probabilities.\n",
        "\n",
        "[Text example of Logistic Regression](https://www.w3schools.com/python/python_ml_logistic_regression.asp)\n",
        "\n",
        "[Youtube tutorial of Logistic Regression](https://www.youtube.com/watch?v=XnOAdxOWXWg&t=12s)"
      ],
      "metadata": {
        "id": "8bw3LbQTn30Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-10T21:35:27.683485Z",
          "iopub.execute_input": "2023-01-10T21:35:27.683902Z",
          "iopub.status.idle": "2023-01-10T21:35:31.666913Z",
          "shell.execute_reply.started": "2023-01-10T21:35:27.683859Z",
          "shell.execute_reply": "2023-01-10T21:35:31.665852Z"
        },
        "trusted": true,
        "id": "YefENmbCMarV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gLYRnvh1LWW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MK2-xRB4LZPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " ## **Medium-level Algorithm Selection**\n",
        "\n",
        "Down below, you will use the usage of XGBoost library, more specifically the **XGBoost Classifer**.\n",
        "\n",
        "**XGBoost** is an open-source gradient boosting library that provides a powerful and efficient framework for training and applying gradient boosting models, capable of handling diverse machine learning tasks with high accuracy and speed.\n",
        "\n",
        "[Video resource for XGboost Library](https://www.youtube.com/watch?v=GrJP9FLV3FE&ab_channel=StatQuestwithJoshStarmer\n",
        ")\n",
        "\n",
        "[Xgboost document text article](https://xgboost.readthedocs.io/en/stable/get_started.html)\n",
        "\n",
        "\n",
        "**XGBoost Classifier** is a specific implementation of the XGBoost library designed for classification tasks, utilizing the gradient boosting framework to build highly accurate and efficient classification models. It offers enhanced performance through various optimization techniques, handling both binary and multiclass classification problems with the flexibility to handle imbalanced datasets and handle missing values.\n",
        "\n",
        "[Xgboost classifier video resource](https://www.youtube.com/watch?v=2Ou7gcqTqBE\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GWvgYnB_9l_E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-10T21:35:31.670647Z",
          "iopub.execute_input": "2023-01-10T21:35:31.671618Z",
          "iopub.status.idle": "2023-01-10T21:35:38.205141Z",
          "shell.execute_reply.started": "2023-01-10T21:35:31.671578Z",
          "shell.execute_reply": "2023-01-10T21:35:38.203784Z"
        },
        "trusted": true,
        "id": "t2StyRRKMarV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nkRxmvBCLbir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "457KT-Y4Lbta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **High Complexity Solution**\n",
        "\n",
        "Down below, the **RandomForest Classifier** is used:\n",
        "\n",
        "Random Forest is an ensemble learning algorithm that combines multiple decision trees to make predictions. It uses random subsets of the training data and features to reduce overfitting and provides accurate and robust predictions for both classification and regression tasks.\n",
        "\n",
        "\n",
        "\n",
        "[Text resource for RandomForest](https://www.ibm.com/topics/random-forest)\n",
        "\n",
        "[Youtube explanation of RandomForest](https://www.youtube.com/watch?v=eM4uJ6XGnSM&t=22s\n",
        ")\n",
        "\n",
        "RandomForestClassifier is a specific implementation of the Random Forest algorithm for classification tasks. It combines the predictions of multiple decision trees, each trained on a random subset of the training data, to produce a reliable and robust classifier. RandomForestClassifier is known for its ability to handle high-dimensional data, feature interactions, and noisy or unbalanced datasets, making it a popular choice for classification problems in machine learning.\n",
        "\n",
        "[Text resource for RandomForest Classifier](https://www.datacamp.com/tutorial/random-forests-classifier-python)\n",
        "\n",
        "[Short video resource for RandomForest Classifier](https://www.youtube.com/watch?v=x9pIM2GkbF4)"
      ],
      "metadata": {
        "id": "AqSmTpVN-Nr_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-10T21:35:38.206862Z",
          "iopub.execute_input": "2023-01-10T21:35:38.207366Z",
          "iopub.status.idle": "2023-01-10T21:39:58.690602Z",
          "shell.execute_reply.started": "2023-01-10T21:35:38.207315Z",
          "shell.execute_reply": "2023-01-10T21:39:58.689465Z"
        },
        "trusted": true,
        "id": "CWoFrSolMarW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wJxGiiQzLdXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oZfC2qqbLdrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Deep Learning Approach**\n",
        "The Next Approach Utilizes **Keras**, a high-level deep learning framework used for building and training neural network models. In classification problems, the approach involves data preparation, defining the model architecture, compiling the model with loss function and optimizer, training the model on the prepared data, and evaluating its performance using metrics like accuracy or F1 score. The model can then be fine-tuned and deployed for making predictions on new data."
      ],
      "metadata": {
        "id": "PjE5xqv79Kwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions For Keras Models"
      ],
      "metadata": {
        "id": "iepCFGjoMarW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **F1 score** is a metric commonly used to evaluate the performance of a binary classification model. It considers both precision and recall to provide a balanced measure of the model's accuracy.\n",
        "\n",
        "* **Precision** measures the proportion of true positive predictions among all positive predictions. It represents the model's ability to correctly identify positive instances. Precision is calculated as the ratio of true positive predictions to the sum of true positive and false positive predictions.\n",
        "\n",
        "* **Recall**, also known as sensitivity or true positive rate, measures the proportion of true positive predictions among all actual positive instances. It represents the model's ability to capture all positive instances. Recall is calculated as the ratio of true positive predictions to the sum of true positive and false negative predictions.\n",
        "* **Additional Resources:**\n",
        "  * [Precision, Recall, & F1 Score Intuitively Explained](https://www.youtube.com/watch?v=8d3JbbSj-I8)\n",
        "  * [What are Precision and Recall in Machine Learning?](https://www.youtube.com/watch?v=FWBoW04gyew)\n",
        "\n",
        "\\\n",
        "**Focal loss** is a specialized loss function used in classification tasks to address class imbalance. By assigning higher weights to misclassified examples, particularly from the minority class (in our case, the minority class is fraudulent transactions), the focal loss function helps the model focus on reducing false positives. The increased emphasis on misclassified instances encourages the model to pay closer attention to the minority class and improve its ability to correctly classify those instances.\n",
        "* **Additional Resources:**\n",
        "  * [Focal Loss — What, Why, and How?\n",
        "  ](https://medium.com/swlh/focal-loss-what-why-and-how-df6735f26616)\n",
        "  * [Focal Loss: A better alternative for Cross-Entropy](https://towardsdatascience.com/focal-loss-a-better-alternative-for-cross-entropy-1d073d92d075)"
      ],
      "metadata": {
        "id": "nmCBf-FX8bDn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-10T21:39:58.69241Z",
          "iopub.execute_input": "2023-01-10T21:39:58.692828Z",
          "iopub.status.idle": "2023-01-10T21:39:58.704863Z",
          "shell.execute_reply.started": "2023-01-10T21:39:58.692788Z",
          "shell.execute_reply": "2023-01-10T21:39:58.703817Z"
        },
        "trusted": true,
        "id": "zKxaLmJAMarW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eu9wJCh2LgSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ANDwiHDLgfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Compilation:** The compilation step in the code sets up the model for training. It defines the metrics that will be used to evaluate the model's performance during training and optimization.\n",
        " * The **loss function**, in this case \"binary_crossentropy,\" measures the difference between the predicted and actual outputs and guides the model to minimize this difference.\n",
        " * The **optimization function**, in this case \"Adam\" with a learning rate of 1e-2, determines how the model's weights are updated based on the loss function's value.\n",
        "\n",
        "* **Training Loop:** The training loop is responsible for actually training the model on the provided dataset. It includes techniques like *EarlyStopping*, which monitors the model's performance during training and stops training if the performance does not improve for a certain number of epochs, thus preventing overfitting. *Class weights* are calculated to address class imbalance, giving more importance to the minority class during training. The model is trained for a specified number of *epochs*, which represent the number of times the entire dataset is passed through the model during training.\n",
        "\n",
        "* **Evaluation:** The evaluation step measures the model's performance on the test dataset. It generates predictions using the trained model on the test data and then evaluates these predictions using the performance metric functions (AUC, etc) discussed earlier.\n",
        "\n",
        "* **Additional Resources:**\n",
        "  * [What are Optimizers in deep learning?](https://www.youtube.com/watch?v=JhQqquVeCE0)\n",
        "  * [What are Loss functions in machine learning?](https://www.youtube.com/watch?v=JhQqquVeCE0)\n",
        "  * [How to choose an optimizer for a Tensorflow Keras model?](https://www.youtube.com/watch?v=pd3QLhx0Nm0)"
      ],
      "metadata": {
        "id": "MUbi_3B5nSz8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-10T21:39:58.706557Z",
          "iopub.execute_input": "2023-01-10T21:39:58.706979Z",
          "iopub.status.idle": "2023-01-10T21:39:58.717875Z",
          "shell.execute_reply.started": "2023-01-10T21:39:58.706939Z",
          "shell.execute_reply": "2023-01-10T21:39:58.716977Z"
        },
        "trusted": true,
        "id": "EGud6W9FMarW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aputUyaiLhtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nFxCUS_HLh18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Machine Learning Layers\n",
        "**Layers** in machine learning are basic units that process input data and contribute to the learning and decision-making process in neural networks.\n",
        "\n",
        "* **Dense**: a dense layer is a fundamental building block of a neural network. It is also known as a fully connected layer or a linear layer. The purpose of a dense layer is to transform the input data by performing a linear operation followed by an activation function.\n",
        "\n",
        "* **Dropout**: a regularization technique that randomly sets a fraction of input units to 0 during training. This means that for each training sample, some neurons in the network are \"dropped out\" or temporarily ignored. **Dropout helps prevent overfitting** by reducing the co-adaptation of neurons. By randomly dropping out neurons, the network becomes more robust and less likely to rely on specific neurons for making predictions, thus improving generalization. During testing or inference, dropout is turned off, and the full network is used for predictions.\n",
        "\n",
        "* **Batch normalization**: a technique used to normalize the inputs of each layer in a neural network. It normalizes the input by subtracting the mini-batch mean and dividing by the mini-batch standard deviation. This helps stabilize and speed up the training process by reducing the internal covariate shift. Additionally, it provides regularization effects by adding some noise to the network and reducing the impact of individual mini-batch examples. Batch Normalization can improve model convergence, allow for higher learning rates, and make the model more robust to changes in input distributions.\n",
        "\n",
        "**Additional Resources**:\n",
        "\n",
        "\n",
        "*   [Introduction to Convolutional Neural Network (CNN) using Tensorflow](https://towardsdatascience.com/introduction-to-convolutional-neural-network-cnn-de73f69c5b83#:~:text=Dense%20Layer%20is%20simple%20layer,multiple%20number%20of%20such%20neurons.)\n",
        "*   [Batch normalization in 3 levels of understanding](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338)\n",
        "*   [Dropout in Neural Networks](https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9)\n",
        "*   [Dense layers explained in a simple way](https://medium.com/datathings/dense-layers-explained-in-a-simple-way-62fe1db0ed75)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rfAA9VSa9fn1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2023-01-10T21:39:58.721064Z",
          "iopub.execute_input": "2023-01-10T21:39:58.723205Z",
          "iopub.status.idle": "2023-01-10T21:43:31.464311Z",
          "shell.execute_reply.started": "2023-01-10T21:39:58.723166Z",
          "shell.execute_reply": "2023-01-10T21:43:31.463354Z"
        },
        "trusted": true,
        "id": "ZhFk_zuVMarW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-10T21:43:31.467395Z",
          "iopub.execute_input": "2023-01-10T21:43:31.467698Z",
          "iopub.status.idle": "2023-01-10T21:44:35.623719Z",
          "shell.execute_reply.started": "2023-01-10T21:43:31.467671Z",
          "shell.execute_reply": "2023-01-10T21:44:35.622627Z"
        },
        "trusted": true,
        "id": "GbD9k3U2MarX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analyzing Model Performance**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Logistic Regression\n",
        "* AUC: 0.8779388959897662\n",
        "* TPR:  49.69%\n",
        "* FPR:  5.0%\n",
        "*Threshold:  0.76\n",
        "\n",
        "The logistic regression model has a relatively high AUC, indicating good performance in distinguishing between positive and negative samples. The TPR of 49.69% suggests that it correctly identifies approximately half of the positive cases, while the FPR of 5.0% implies that it misclassifies 5.0% of the negative cases as positive. The chosen threshold of 0.76 determines the classification boundary.\n",
        "\n",
        "## XGBoost\n",
        "* AUC: 0.7721620028119188\n",
        "* TPR:  17.65%\n",
        "* FPR:  2.85%\n",
        "* Threshold:  0.0\n",
        "\n",
        "The XGBoost model has a lower AUC compared to logistic regression, indicating a somewhat weaker performance. The TPR of 17.65% suggests that it captures a smaller proportion of the positive cases, while the FPR of 2.85% indicates a lower rate of false positives. The chosen threshold of 0.0 suggests a sensitive classification boundary.\n",
        "\n",
        "## Random Forest\n",
        "* AUC: 0.6313642342056329\n",
        "* TPR:  0.0%\n",
        "* FPR:  1.0699999999999998%\n",
        "* Threshold:  0.02\n",
        "\n",
        "The random forest model has the lowest AUC among the analyzed models, indicating the weakest discriminatory power. The TPR of 0.0% suggests that it fails to identify any positive cases, which is a significant limitation. However, the FPR of 1.07% indicates a relatively low rate of false positives.\n",
        "\n",
        "## Keras\n",
        "* AUC: 0.6824300421787837\n",
        "* TPR:  23.53%\n",
        "* FPR:  2.65%\n",
        "* Threshold:  0.76\n",
        "\n",
        "The Keras model's AUC falls between that of XGBoost and random forest, indicating moderate performance. The TPR of 23.53% suggests it captures a larger proportion of positive cases compared to XGBoost. The FPR of 2.65% indicates a relatively low rate of false positives. Similar to logistic regression, the chosen threshold of 0.76 determines the classification boundary for this model.\n",
        "\n",
        "##Best Overall Performance\n",
        "Overall, based on the provided metrics, the logistic regression model appears to have the best performance among the analyzed models, with the highest AUC and relatively balanced TPR and FPR. However, it's important to note that the interpretation and comparison of model performance should take into consideration the specific context of deployment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KOlWnsQQlS5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "- Hyperparameter tuning for the models"
      ],
      "metadata": {
        "id": "OEuUfTzkMarX"
      }
    }
  ]
}